{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "181e36ec",
   "metadata": {},
   "source": [
    "# Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4c443c7e",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (4.11.1)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from beautifulsoup4) (2.3.1)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "18d33dba",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (4.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "402c4b76",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (3.7)\n",
      "Requirement already satisfied: click in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from nltk) (4.64.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38022161",
   "metadata": {},
   "source": [
    "# Import Liberaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "58475c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f2a299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "912ffdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "57d4b1d5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2588e677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1de95773",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc5e575a",
   "metadata": {},
   "source": [
    "# Extract Text from HTML Document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7f871f04",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_html(file_path):\n",
    "    with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "        soup = BeautifulSoup(file, 'html.parser')\n",
    "        text = soup.get_text(separator=' ', strip=True)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa618bdb",
   "metadata": {},
   "source": [
    "# Clean Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "bd0f2ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a590c0",
   "metadata": {},
   "source": [
    "# Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "61fd0be0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ccc8b86",
   "metadata": {},
   "source": [
    "# Segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "bf21090a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_text(text):\n",
    "    sentences = sent_tokenize(text)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a02c85cf",
   "metadata": {},
   "source": [
    "# Save Sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "f156aaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_sentences(sentences, output_path):\n",
    "    os.makedirs(os.path.dirname(output_path), exist_ok=True)\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        for sentence in sentences:\n",
    "            f.write(sentence + '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4439b25d",
   "metadata": {},
   "source": [
    "# Complete Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "f2d87ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_transcripts(root_folder, output_root):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.html'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                text = extract_text_from_html(file_path)\n",
    "                sentences=segment_text(text)\n",
    "                cleaned_sentences = [clean_text(sentence) for sentence in sentences]\n",
    "                normalized_sentences = [normalize_text(sentence) for sentence in cleaned_sentences]\n",
    "                sentences=normalized_sentences\n",
    "                relative_path = os.path.relpath(root, root_folder)\n",
    "                output_dir = os.path.join(output_root, relative_path)\n",
    "                output_file = os.path.splitext(file)[0] + '.txt'\n",
    "                output_path = os.path.join(output_dir, output_file)\n",
    "                save_sentences(sentences, output_path)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8214b96e",
   "metadata": {},
   "source": [
    "# Define Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1c118e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'Data'\n",
    "output_root = 'Segmented_Sentences'\n",
    "if not os.path.exists(output_root):\n",
    "    os.makedirs(output_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbc3a320",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_transcripts(root_folder, output_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1199d612",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "461e1197",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.6.2)\n",
      "Requirement already satisfied: jinja2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (63.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.24.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.9)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "dfc25eb1",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.7.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
      "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
      "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
      "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.0)\n",
      "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (5.2.1)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.64.1)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.28.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.6.2)\n",
      "Requirement already satisfied: jinja2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.11.3)\n",
      "Requirement already satisfied: setuptools in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (63.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (21.3)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.19.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.24.2)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.9.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2022.9.24)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.0.4)\n",
      "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.1)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7ccedd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def remove_stop_words(text):\n",
    "    doc = nlp(text)\n",
    "    # Join the tokens that are not stop words\n",
    "    filtered_text = \" \".join([token.text for token in doc if not token.is_stop])\n",
    "    return filtered_text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "25c0c812",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_texts_remove_stopwords(source_dir, target_dir):\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                # Construct the source file path\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Read the content of the source file\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                # Remove stop words from the content\n",
    "                cleaned_text = remove_stop_words(text)\n",
    "                \n",
    "                # Construct the target file path\n",
    "                relative_path = os.path.relpath(root, source_dir)\n",
    "                target_file_path = os.path.join(target_dir, relative_path, file)\n",
    "                \n",
    "                # Ensure the target directory exists\n",
    "                os.makedirs(os.path.dirname(target_file_path), exist_ok=True)\n",
    "                with open(target_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write(cleaned_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8d95f499",
   "metadata": {},
   "outputs": [],
   "source": [
    "source_directory = 'Segmented_Sentences'\n",
    "target_directory = 'cleaned_lectures'\n",
    "process_texts_remove_stopwords(source_directory, target_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a439c3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38dc2cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a48446",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d3e74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c41e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "39f77b80",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: scikit-learn in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (1.1.1)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.24.2)\n",
      "Requirement already satisfied: scipy>=1.3.2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.0.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from scikit-learn) (2.2.0)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5fa1bf69",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def extract_keywords_tfidf(documents, top_n=50):\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(documents)\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    keywords_list = []\n",
    "    for doc_id in range(X.shape[0]):\n",
    "        feature_index = X[doc_id,:].nonzero()[1]\n",
    "        tfidf_scores = zip([feature_names[i] for i in feature_index], [X[doc_id, x] for x in feature_index])\n",
    "        sorted_items = sorted(tfidf_scores, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        keywords_list.append([item[0] for item in sorted_items])\n",
    "    return keywords_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8d5d79c5",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting rake-nltk\n",
      "  Downloading rake_nltk-1.0.6-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: nltk<4.0.0,>=3.6.2 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from rake-nltk) (3.7)\n",
      "Requirement already satisfied: click in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (8.0.4)\n",
      "Requirement already satisfied: joblib in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (1.2.0)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (2022.7.9)\n",
      "Requirement already satisfied: tqdm in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from nltk<4.0.0,>=3.6.2->rake-nltk) (4.64.1)\n",
      "Downloading rake_nltk-1.0.6-py3-none-any.whl (9.1 kB)\n",
      "Installing collected packages: rake-nltk\n",
      "Successfully installed rake-nltk-1.0.6\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.3.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.0\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install rake-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c064efc7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "6fbece34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_and_extract_keywords(source_dir, target_dir, top_n_keywords=50):\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        # Determine the path for output based on the current directory structure\n",
    "        rel_path = os.path.relpath(root, source_dir)\n",
    "        output_dir = os.path.join(target_dir, rel_path)\n",
    "        \n",
    "        # Ensure the output directory exists\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = [f.read()]\n",
    "                \n",
    "                # Extract keywords for the document\n",
    "                keywords = extract_keywords_tfidf(text, top_n=top_n_keywords)[0]\n",
    "                \n",
    "                # Construct the output filename and write the keywords\n",
    "                output_file_path = os.path.join(output_dir, os.path.splitext(file)[0] + '_keywords.txt')\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(keywords))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "7c6978ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your source and target directories\n",
    "source_directory = 'cleaned_lectures'\n",
    "target_directory = 'keywords'\n",
    "\n",
    "process_and_extract_keywords(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0aa620e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "65541412",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from rake_nltk import Rake\n",
    "import os\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Extend the default NLTK stop words list with additional words as needed\n",
    "extended_stopwords = set(stopwords.words('english'))\n",
    "# Add 'got' and any other words you deem necessary\n",
    "extended_stopwords.update(['got', 'like', 'just', 'know', 'think', 'see', 'really', 'said'])\n",
    "\n",
    "# Preprocess the document to remove numbers\n",
    "def preprocess_document(document):\n",
    "    # Remove numbers\n",
    "    document_no_numbers = re.sub(r'\\b\\d+\\b', '', document)\n",
    "    return document_no_numbers\n",
    "\n",
    "def extract_keywords_rake(document, top_n=50, min_length=4, max_length=20, min_words=2, max_words=4):\n",
    "    # Preprocess document to remove numbers\n",
    "    document = preprocess_document(document)\n",
    "    \n",
    "    # Initialize RAKE with the extended list of stopwords\n",
    "    rake = Rake(stopwords=extended_stopwords, min_length=min_length, max_length=max_length)\n",
    "    \n",
    "    # Extract keywords from the document\n",
    "    rake.extract_keywords_from_text(document)\n",
    "    \n",
    "    # Get keyword phrases ranked highest to lowest with scores\n",
    "    ranked_phrases_with_scores = rake.get_ranked_phrases_with_scores()\n",
    "    \n",
    "    # Filter phrases based on the number of words\n",
    "    filtered_phrases = [phrase for score, phrase in ranked_phrases_with_scores if min_words <= len(phrase.split()) <= max_words]\n",
    "    \n",
    "    # Select the top_n phrases based on the adjusted list\n",
    "    top_keywords = filtered_phrases[:top_n]\n",
    "    \n",
    "    return top_keywords\n",
    "\n",
    "\n",
    "def process_and_extract_keywords_rake(source_dir, target_dir, top_n_keywords=50, min_length=3, max_length=20, min_words=1, max_words=3):\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        rel_path = os.path.relpath(root, source_dir)\n",
    "        output_dir = os.path.join(target_dir, rel_path)\n",
    "        \n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        for file in files:\n",
    "            if file.endswith('.txt'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    text = f.read()\n",
    "                \n",
    "                # Extract keywords for the document\n",
    "                keywords = extract_keywords_rake(text, top_n=top_n_keywords, min_length=min_length, max_length=max_length, min_words=min_words, max_words=max_words)\n",
    "                \n",
    "                # Construct the output filename and write the keywords\n",
    "                output_file_path = os.path.join(output_dir, os.path.splitext(file)[0] + '_keywords.txt')\n",
    "                with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "                    f.write('\\n'.join(keywords))\n",
    "\n",
    "# Define your source and target directories\n",
    "source_directory = 'cleaned_lectures'  # Update this path as necessary\n",
    "target_directory = 'keywords_rake'  # Update this path as necessary\n",
    "\n",
    "process_and_extract_keywords_rake(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69818abf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a6da59c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f11dcbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131b0e2b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b49d5b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f01afc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659fab30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96fc8204",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35375b9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ccd806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b17797",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287da3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a1f2b96",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "278477e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86e413a4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf9d6b22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c00bc371",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "249aa5c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3dec8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e865db3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6900e50f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f9242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "735b4d52",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "339d0b47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "# Load the spaCy English model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def perform_ner(keywords):\n",
    "    # Join keywords into a single text string for NER\n",
    "    text = \" \".join(keywords)\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "221d863b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def process_keyword_files_for_ner(source_dir, target_dir):\n",
    "    for root, dirs, files in os.walk(source_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('_keywords.txt'):  # Assuming your keyword files have a specific naming pattern\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Read the keywords from the file\n",
    "                with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                    keywords = f.read().splitlines()\n",
    "                \n",
    "                # Perform NER on the list of keywords\n",
    "                entities = perform_ner(keywords)\n",
    "                \n",
    "                # Construct the target file path to save entities\n",
    "                relative_path = os.path.relpath(root, source_dir)\n",
    "                target_file_path = os.path.join(target_dir, relative_path, os.path.splitext(file)[0] + '_entities.txt')\n",
    "                \n",
    "                # Ensure the target directory exists\n",
    "                os.makedirs(os.path.dirname(target_file_path), exist_ok=True)\n",
    "                \n",
    "                # Save the entities to the target file\n",
    "                with open(target_file_path, 'w', encoding='utf-8') as f:\n",
    "                    for entity, label in entities:\n",
    "                        f.write(f\"{entity} ({label})\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "18b6116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Specify your source directory (where the keyword files are) and target directory (where you want to save entities)\n",
    "source_directory = 'keywords'\n",
    "target_directory = 'NER'\n",
    "\n",
    "process_keyword_files_for_ner(source_directory, target_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c24a04f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted Keywords:\n",
      "beauty\n",
      "beautiful\n",
      "beautifully\n",
      "civil\n",
      "civilization\n",
      "civilized\n",
      "sexuality\n",
      "sexual\n",
      "sexually\n",
      "united\n"
     ]
    }
   ],
   "source": [
    "from summa import keywords as summa_keywords\n",
    "\n",
    "def extract_keywords_textrank(text, top_n=10):\n",
    "    try:\n",
    "        # Attempt to extract top_n keywords with scores\n",
    "        tr_keywords = summa_keywords.keywords(text, words=top_n, split=True, scores=True)\n",
    "        tr_keywords_sorted = sorted(tr_keywords, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        return [keyword for keyword, score in tr_keywords_sorted]\n",
    "    except IndexError:\n",
    "        # If an IndexError is raised, adjust to return available keywords\n",
    "        tr_keywords = summa_keywords.keywords(text, split=True, scores=True)\n",
    "        tr_keywords_sorted = sorted(tr_keywords, key=lambda x: x[1], reverse=True)\n",
    "        return [keyword for keyword, score in tr_keywords_sorted][:top_n]\n",
    "\n",
    "def main():\n",
    "    # Path to your text file\n",
    "    file_path = '/Users/saicharangankidi/Desktop/shruthiProject/Sentences/afm162/transcripts/transcript01.txt'  # Update this to the path of your text file\n",
    "\n",
    "    # Read the content of the text file\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        text_input = file.read()\n",
    "\n",
    "    # Extract keywords\n",
    "    extracted_keywords = extract_keywords_textrank(text_input, top_n=10)\n",
    "\n",
    "    # Print extracted keywords\n",
    "    print(\"Extracted Keywords:\")\n",
    "    for keyword in extracted_keywords:\n",
    "        print(keyword)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e565c4df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
