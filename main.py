# -*- coding: utf-8 -*-
"""shruthiProject8April-2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wEPbttzyq0TS4KtoE6bdv8DPWN8Y1RUm
"""
#pipimport gradio as gr
import os
from bs4 import BeautifulSoup
import nltk
from nltk.tokenize import word_tokenize
from nltk.corpus import stopwords, words
from nltk.stem import WordNetLemmatizer
from nltk import pos_tag
from nltk.corpus import wordnet
import re
import networkx as nx
import matplotlib.pyplot as plt
import io
from wordcloud import WordCloud
from PIL import Image
import plotly.graph_objects as go
import nltk
import ssl

try:
    _create_unverified_https_context = ssl._create_unverified_context
except AttributeError:
    # Legacy Python that doesn't verify HTTPS certificates by default
    pass
else:
    # Handle target environment that doesn't support HTTPS verification
    ssl._create_default_https_context = _create_unverified_https_context

# Downloading necessary NLTK resources
nltk.download('omw-1.4', quiet=True)
nltk.download('punkt', quiet=True)
nltk.download('stopwords', quiet=True)
nltk.download('averaged_perceptron_tagger', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('words', quiet=True)

# Set of English words from NLTK
english_words = set(words.words())

def get_wordnet_pos(treebank_tag):
    """Convert treebank POS tags to WordNet POS tags."""
    if treebank_tag.startswith('J'):
        return wordnet.ADJ
    elif treebank_tag.startswith('V'):
        return wordnet.VERB
    elif treebank_tag.startswith('N'):
        return wordnet.NOUN
    elif treebank_tag.startswith('R'):
        return wordnet.ADV
    else:
        return wordnet.NOUN  # Default to noun if no match

def process_html_file(file_path):
    soup = BeautifulSoup(file_path, 'html.parser')
    text = soup.get_text(separator=' ', strip=True)
    return text

def preprocess_text(text):
    # Preprocess text
    text = re.sub(r'http\S+', '', text)
    text = re.sub(r'\s+', ' ', text)
    text = re.sub(r'[^\w\s]', '', text)
    words = word_tokenize(text)
    words = [word.lower() for word in words]
    stop_words = set(stopwords.words('english'))
    words = [word for word in words if word not in stop_words and word.isalpha()]
    words = [word for word in words if word in english_words]
    tagged_words = pos_tag(words)
    lemmatizer = WordNetLemmatizer()
    words_lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_words]
    preprocessed_text = ' '.join(words_lemmatized)

    # Here you could integrate keyword extraction and puzzle generation
    return preprocessed_text

from sklearn.feature_extraction.text import TfidfVectorizer
from summa import keywords as summa_keywords

def extract_combined_keywords(text, top_n=10):
    # Extract keywords using TF-IDF
    tfidf_vectorizer = TfidfVectorizer(stop_words='english')
    tfidf_matrix = tfidf_vectorizer.fit_transform([text])
    feature_names = tfidf_vectorizer.get_feature_names_out()
    tfidf_scores = tfidf_matrix.toarray().flatten()
    top_indices_tfidf = tfidf_scores.argsort()[-top_n:][::-1]
    keywords_tfidf = [feature_names[index] for index in top_indices_tfidf]

    # Extract keywords using TextRank
    keywords_textrank = summa_keywords.keywords(text, words=top_n, split=True, scores=False)

    # Combine and deduplicate keywords
    combined_keywords = list(set(keywords_tfidf + keywords_textrank))

    return combined_keywords


# Use this function to extract keywords after preprocessing your text in the Gradio app.
# Then, you can utilize these keywords for generating your interactive puzzles.

def generate_graph(keywords, text):
    """Generate a network graph of keyword co-occurrence in the text."""
    G = nx.Graph()
    for keyword in keywords:
        G.add_node(keyword)
    for i, word1 in enumerate(keywords):
        for j, word2 in enumerate(keywords[i+1:], i+1):
            if text.count(word1) > 0 and text.count(word2) > 0:
                G.add_edge(word1, word2, weight=1)

    pos = nx.spring_layout(G, k=0.5)
    plt.figure(figsize=(10, 10))
    nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=2500, edge_color='k', linewidths=1, font_size=10, alpha=0.6)
    plt.title("Keyword Co-occurrence Graph", size=15)

    img_buf = io.BytesIO()
    plt.savefig(img_buf, format='png', dpi=150)
    plt.close()
    img_buf.seek(0)
    img = Image.open(img_buf)
    return img

def generate_interactive_graph(keywords, text):
    """Generate an interactive network graph of keyword co-occurrence."""
    G = nx.Graph()
    for keyword in keywords:
        G.add_node(keyword)
    for i, word1 in enumerate(keywords):
        for j, word2 in enumerate(keywords[i+1:], i+1):
            if text.count(word1) > 0 and text.count(word2) > 0:
                G.add_edge(word1, word2, weight=1)

    pos = nx.spring_layout(G, k=0.5)
    edge_x = []
    edge_y = []
    for edge in G.edges():
        x0, y0 = pos[edge[0]]
        x1, y1 = pos[edge[1]]
        edge_x.append(x0)
        edge_x.append(x1)
        edge_x.append(None)
        edge_y.append(y0)
        edge_y.append(y1)
        edge_y.append(None)

    edge_trace = go.Scatter(x=edge_x, y=edge_y, line=dict(width=0.5, color='#888'), hoverinfo='none', mode='lines')

    node_x = []
    node_y = []
    text = []
    for node in G.nodes():
        x, y = pos[node]
        node_x.append(x)
        node_y.append(y)
        text.append(node)

    node_trace = go.Scatter(x=node_x, y=node_y, text=text, mode='markers+text', hoverinfo='text', marker=dict(showscale=True, colorscale='YlGnBu', color=[], size=10, colorbar=dict(thickness=15, title='Node Connections', xanchor='left', titleside='right')))

    node_adjacencies = []
    node_text = []
    for node, adjacencies in enumerate(G.adjacency()):
        node_adjacencies.append(len(adjacencies[1]))
        node_text.append('# of connections: '+str(len(adjacencies[1])))

    node_trace.marker.color = node_adjacencies
    node_trace.text = node_text

    fig = go.Figure(data=[edge_trace, node_trace], layout=go.Layout(showlegend=False, hovermode='closest', margin=dict(b=0,l=0,r=0,t=0), xaxis=dict(showgrid=False, zeroline=False, showticklabels=False), yaxis=dict(showgrid=False, zeroline=False, showticklabels=False)))

    return fig

def process_and_visualize(file_obj):
    """Process the uploaded file, extract text, generate keywords, and visualize relationships."""
    text = process_html_file(file_obj)
    processed_text =preprocess_text(text)
    keywords = extract_combined_keywords(processed_text)
    img = generate_graph(keywords, text)
    fig = generate_interactive_graph(keywords, text)
    return text,img,fig


from flask import Flask, request, render_template, jsonify
import os
from werkzeug.utils import secure_filename

app = Flask(__name__)
app.config['UPLOAD_FOLDER'] = 'uploads/'  # Define the upload folder
app.config['ALLOWED_EXTENSIONS'] = {'html', 'htm'}

# Create the uploads directory if it doesn't already exist
os.makedirs(app.config['UPLOAD_FOLDER'], exist_ok=True)

def allowed_file(filename):
    return '.' in filename and filename.rsplit('.', 1)[1].lower() in app.config['ALLOWED_EXTENSIONS']

@app.route('/')
def index():
    return render_template('index.html')

@app.route('/game')
def game():
    return render_template('game.html')

@app.route('/process-html', methods=['POST'])
def process_html():
    if 'file' not in request.files:
        return jsonify({"error": "No file part"}), 400
    file = request.files['file']
    if file.filename == '':
        return jsonify({"error": "No selected file"}), 400
    if file and allowed_file(file.filename):
        filename = secure_filename(file.filename)
        file_path = os.path.join(app.config['UPLOAD_FOLDER'], filename)
        file.save(file_path)  # Save the file first

        with open(file_path, 'r', encoding='utf-8') as file:
            file_content = file.read()

        soup = BeautifulSoup(file_content, 'html.parser')
        text = soup.get_text(separator=' ', strip=True)
        processed_text = preprocess_text(text)
        keywords = extract_combined_keywords(processed_text)
        top_keywords = keywords[:9] 
        return jsonify({"text": processed_text,"top_keywords": top_keywords, "showGameLink": True})  # Indicating to show the game link
    else:
        return jsonify({"error": "Invalid file type"}), 400

if __name__ == '__main__':
    app.run(debug=True)
