{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "540fe45c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (24.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbda1700",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.11/site-packages (4.12.2)\r\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.11/site-packages (from beautifulsoup4) (2.5)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b7efa64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: chardet in /opt/anaconda3/lib/python3.11/site-packages (4.0.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install chardet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c42daefa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /opt/anaconda3/lib/python3.11/site-packages (3.8.1)\r\n",
      "Requirement already satisfied: click in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (8.1.7)\r\n",
      "Requirement already satisfied: joblib in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (1.2.0)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (2023.10.3)\r\n",
      "Requirement already satisfied: tqdm in /opt/anaconda3/lib/python3.11/site-packages (from nltk) (4.65.0)\r\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33750648",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: summa in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (1.2.0)\n",
      "Requirement already satisfied: scipy>=0.19 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from summa) (1.9.1)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.18.5 in /Users/saicharangankidi/opt/anaconda3/lib/python3.9/site-packages (from scipy>=0.19->summa) (1.24.2)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install summa\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07cc74bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk import pos_tag\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('stopwords')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6d1b2f13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def extract_text_from_html(file_path, element=None):\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='ISO-8859-1') as file:\n",
    "            soup = BeautifulSoup(file, 'html.parser')\n",
    "            if element:\n",
    "                texts = [elem.get_text(separator=' ', strip=True) for elem in soup.find_all(element)]\n",
    "                text = ' '.join(texts)\n",
    "            else:\n",
    "                text = soup.get_text(separator=' ', strip=True)\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab4d3eb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, words\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "import re\n",
    "\n",
    "# Ensure necessary NLTK resources are downloaded\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('words')  # Download the words corpus\n",
    "\n",
    "# Load the set of English words from NLTK\n",
    "english_words = set(words.words())\n",
    "\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    \"\"\"\n",
    "    Convert treebank POS tags to WordNet POS tags.\n",
    "    \"\"\"\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return wordnet.ADJ\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return wordnet.VERB\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return wordnet.NOUN\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return wordnet.ADV\n",
    "    else:\n",
    "        return wordnet.NOUN  # Default to noun if no match\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "    # Normalize whitespace\n",
    "    text = re.sub(r'\\s+', ' ', text)\n",
    "    # Remove non-alphanumeric characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    \n",
    "    # Tokenize into words\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Lowercase\n",
    "    words = [word.lower() for word in words]\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    # Add custom words to the stop words set\n",
    "    custom_stop_words = ['include', 'u', 'de']\n",
    "    stop_words.update(custom_stop_words)\n",
    "    words = [word for word in words if word not in stop_words and word.isalpha()]\n",
    "    \n",
    "    # Remove words that are not in the English dictionary\n",
    "    words = [word for word in words if word in english_words]\n",
    "    \n",
    "    # POS tagging\n",
    "    tagged_words = pos_tag(words)\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    words_lemmatized = [lemmatizer.lemmatize(word, get_wordnet_pos(tag)) for word, tag in tagged_words]\n",
    "    words_lemmatize = [word for word in words_lemmatized if word not in stop_words and word.isalpha()]\n",
    "    \n",
    "    return words_lemmatize\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8fba28e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates_preserve_order(words):\n",
    "    unique_words = []\n",
    "    for word in words:\n",
    "        if word not in unique_words:\n",
    "            unique_words.append(word)\n",
    "    return unique_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d113bef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_text(text, output_path):\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        f.write(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e60f6b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Make sure to define or import your `extract_text_from_html`, `preprocess_text`, `remove_duplicates_preserve_order`, and `save_sentences` functions here\n",
    "\n",
    "def process_transcripts(root_folder, output_root):\n",
    "    english_words_set = set(nltk.corpus.words.words())\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for file in files:\n",
    "            if file.endswith('.html'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                \n",
    "                # Extract text from HTML\n",
    "                text = extract_text_from_html(file_path)\n",
    "                \n",
    "                # Preprocess the entire text\n",
    "                processed_words = preprocess_text(text)  # Preprocess the entire text at once\n",
    "                \n",
    "                # Remove duplicates from the list of words\n",
    "                unique_words = remove_duplicates_preserve_order(processed_words)  # Remove duplicates\n",
    "                \n",
    "                filtered_words = [word for word in unique_words if word in english_words_set]\n",
    "                \n",
    "                # Reconstruct the text from unique words\n",
    "                processed_text = ' '.join(filtered_words)  # Reconstruct the text\n",
    "                \n",
    "                # Define output directory and file path\n",
    "                relative_path = os.path.relpath(root, root_folder)\n",
    "                output_dir = os.path.join(output_root, relative_path)\n",
    "                os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "                output_file = os.path.splitext(file)[0] + '.txt'\n",
    "                output_path = os.path.join(output_dir, output_file)\n",
    "                \n",
    "                # Save processed text to file\n",
    "                save_text(processed_text, output_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aacb70d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "root_folder = 'Data'\n",
    "output_root = 'Sentences'\n",
    "if not os.path.exists(output_root):\n",
    "    os.makedirs(output_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73fa6d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "process_transcripts(root_folder, output_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5409be0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_folder = 'Sentences'  # Make sure this points to your 'sentences' directory\n",
    "documents = []  # Initialize the list to hold document texts\n",
    "\n",
    "# Walk through the directory and read each text file\n",
    "for root, dirs, files in os.walk(sentences_folder):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "        except UnicodeDecodeError:\n",
    "            # If a UnicodeDecodeError occurs, try a different encoding or ignore errors\n",
    "            with open(file_path, 'r', encoding='latin-1', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "        \n",
    "        documents.append(text)\n",
    "\n",
    "# Now 'documents' contains the preprocessed text of each document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc395de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "def extract_keywords_tfidf(docs, top_n=10):\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(docs)\n",
    "    \n",
    "    # Get feature names to use as output\n",
    "    feature_names = tfidf_vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Extract top n keywords from each document\n",
    "    keywords = []\n",
    "    for doc in range(tfidf_matrix.shape[0]):\n",
    "        scores = tfidf_matrix[doc].toarray().flatten()\n",
    "        sorted_indices = scores.argsort()[-top_n:][::-1]\n",
    "        doc_keywords = [feature_names[index] for index in sorted_indices]\n",
    "        keywords.append(doc_keywords)\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7069f063",
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import keywords as summa_keywords\n",
    "\n",
    "def extract_keywords_textrank(text, top_n=10):\n",
    "    try:\n",
    "        # Attempt to extract top_n keywords with scores\n",
    "        tr_keywords = summa_keywords.keywords(text, words=top_n, split=True, scores=True)\n",
    "        tr_keywords_sorted = sorted(tr_keywords, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        return [keyword for keyword, score in tr_keywords_sorted]\n",
    "    except IndexError:\n",
    "        # If an IndexError is raised, adjust to return available keywords\n",
    "        tr_keywords = summa_keywords.keywords(text, split=True, scores=True)\n",
    "        tr_keywords_sorted = sorted(tr_keywords, key=lambda x: x[1], reverse=True)\n",
    "        return [keyword for keyword, score in tr_keywords_sorted][:top_n]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e1d04432",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def extract_keywords_lda(docs, n_topics=5, top_n=10):\n",
    "    count_vectorizer = CountVectorizer(stop_words='english')\n",
    "    count_data = count_vectorizer.fit_transform(docs)\n",
    "    \n",
    "    lda = LatentDirichletAllocation(n_components=n_topics, random_state=0)\n",
    "    lda.fit(count_data)\n",
    "    \n",
    "    keywords = []\n",
    "    words = count_vectorizer.get_feature_names_out()\n",
    "    for topic_idx, topic in enumerate(lda.components_):\n",
    "        top_features_ind = topic.argsort()[-top_n:][::-1]\n",
    "        topic_keywords = [words[i] for i in top_features_ind]\n",
    "        keywords.append(topic_keywords)\n",
    "    return keywords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8803aa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     /Users/saicharangankidi/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "\n",
    "def extract_keywords_ner(text):\n",
    "    words = word_tokenize(text)\n",
    "    tagged_words = nltk.pos_tag(words)\n",
    "    tree = ne_chunk(tagged_words)\n",
    "    \n",
    "    return set(' '.join(i[0] for i in t) for t in tree if hasattr(t, 'label') and t.label() in ['PERSON', 'ORGANIZATION', 'GPE', 'LOCATION'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb2cb559",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences_folder = 'Sentences'  # Adjust this to the correct path\n",
    "documents = []  # This will store the content of each document\n",
    "file_paths = []  # This will store the path of each document\n",
    "\n",
    "# Walk through the directory and read each text file\n",
    "for root, dirs, files in os.walk(sentences_folder):\n",
    "    for file in files:\n",
    "        file_path = os.path.join(root, file)\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "            documents.append(text)\n",
    "            file_paths.append(file_path)\n",
    "        except UnicodeDecodeError:\n",
    "            # If a UnicodeDecodeError occurs, try reading with a different encoding\n",
    "            with open(file_path, 'r', encoding='latin-1', errors='ignore') as f:\n",
    "                text = f.read()\n",
    "            documents.append(text)\n",
    "            file_paths.append(file_path)\n",
    "\n",
    "# Now 'documents' contains the content of each document\n",
    "# and 'file_paths' contains the corresponding file paths\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "57f11cf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'method_dirs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [20]\u001b[0m, in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;66;03m# Iterate over each method and save the keywords\u001b[39;00m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m method, keywords \u001b[38;5;129;01min\u001b[39;00m method_keywords\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m---> 27\u001b[0m     output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_root, \u001b[43mmethod_dirs\u001b[49m[method])\n\u001b[1;32m     28\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_dir, exist_ok\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Ensure the output directory exists\u001b[39;00m\n\u001b[1;32m     29\u001b[0m     output_file_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplitext(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mbasename(file_path))[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmethod\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'method_dirs' is not defined"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Assuming 'documents' is a list of texts and 'file_paths' is a list of corresponding file paths\n",
    "output_root = 'keywords_output'  # Base directory for saving keywords\n",
    "\n",
    "if not os.path.exists(output_root):\n",
    "    os.makedirs(output_root)\n",
    "\n",
    "\n",
    "for file_path, doc in zip(file_paths, documents):\n",
    "    # Extract keywords with each method\n",
    "    keywords_tfidf = extract_keywords_tfidf([doc])[0]  # Assuming this function returns a list of lists of keywords\n",
    "    keywords_textrank = extract_keywords_textrank(doc)  # Assuming this function returns a list of keywords\n",
    "    keywords_lda = extract_keywords_lda([doc])[0]  # Assuming this function returns a list of lists of keywords for topics\n",
    "    keywords_ner = list(extract_keywords_ner(doc))  # Converting set to list for uniformity\n",
    "\n",
    "    # Organize keywords by method\n",
    "    method_keywords = {\n",
    "        'tfidf': keywords_tfidf,\n",
    "        'textrank': keywords_textrank,\n",
    "        'lda': keywords_lda,\n",
    "        'ner': keywords_ner\n",
    "    }\n",
    "\n",
    "    # Iterate over each method and save the keywords\n",
    "    for method, keywords in method_keywords.items():\n",
    "        output_dir = os.path.join(output_root, method_dirs[method])\n",
    "        os.makedirs(output_dir, exist_ok=True)  # Ensure the output directory exists\n",
    "        output_file_name = f\"{os.path.splitext(os.path.basename(file_path))[0]}_{method}.txt\"\n",
    "        output_file_path = os.path.join(output_dir, output_file_name)\n",
    "\n",
    "        # Save keywords to file\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "            if method == 'lda':  # Special handling for LDA output, which is a list of lists\n",
    "                for topic_keywords in keywords:\n",
    "                    f.write(', '.join(topic_keywords) + '\\n\\n')\n",
    "            else:\n",
    "                f.write('\\n'.join(keywords) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22391f7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "documents[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb9f451",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fb3448",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_keywords=keywords_tfidf+keywords_textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51117344",
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b6137a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import itertools\n",
    "combined_keywords = list(set(keywords_tfidf + keywords_textrank))\n",
    "co_occurrences = defaultdict(int)\n",
    "for doc in documents:\n",
    "    sentences = nltk.sent_tokenize(doc)\n",
    "    for sentence in sentences:\n",
    "        sentence_keywords = [kw for kw in combined_keywords if kw in sentence]\n",
    "        for pair in itertools.combinations(set(sentence_keywords), 2):\n",
    "            co_occurrences[pair] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "747ef1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "co_occurrences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "603e8b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "G = nx.Graph()\n",
    "for (kw1, kw2), count in co_occurrences.items():\n",
    "    G.add_edge(kw1, kw2, weight=count)\n",
    "plt.figure(figsize=(12, 12))\n",
    "pos = nx.spring_layout(G, k=0.5)  # Layout for visualizing the graph's structure\n",
    "nx.draw_networkx(G, pos, node_size=20, edge_color='gray', alpha=0.5, with_labels=True, font_size=10)\n",
    "plt.title(\"Keyword Co-occurrence Graph\", size=15)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40601e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "G = nx.Graph()\n",
    "for pair, weight in co_occurrences.items():\n",
    "    G.add_edge(pair[0], pair[1], weight=weight)\n",
    "pos = nx.spring_layout(G, k=0.1, iterations=50)\n",
    "\n",
    "# Drawing nodes and edges\n",
    "nx.draw_networkx_nodes(G, pos, node_size=20, node_color='blue', alpha=0.6)\n",
    "nx.draw_networkx_edges(G, pos, width=0.25, alpha=1)\n",
    "nx.draw_networkx_labels(G, pos, font_size=8, font_family='sans-serif')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70b6a006",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = dict(G.degree())\n",
    "sorted_degrees = sorted(degrees.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"Top 10 central keywords:\", sorted_degrees[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c923496",
   "metadata": {},
   "outputs": [],
   "source": [
    "from networkx.algorithms.community import greedy_modularity_communities\n",
    "\n",
    "communities = list(greedy_modularity_communities(G))\n",
    "for i, comm in enumerate(communities, 1):\n",
    "    print(f\"Community {i}: \", list(comm)[:10])  # Displaying first 10 keywords of each community\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c42c6eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortest_path = nx.shortest_path(G, source='good', target='great', weight='weight')\n",
    "print(\"Shortest path from 'good' to 'great':\", shortest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8043cd61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your shortest path data\n",
    "shortest_path = ['good', 'reflection', 'goal', 'great']\n",
    "\n",
    "# Sample text for the fill-in-the-blanks puzzle\n",
    "text_template = \"\"\"\n",
    "1. The journey of self-improvement is always ongoing, striving from being ___1___ to achieving greatness.\n",
    "2. A moment of ___2___ can sometimes show us the direction we need to take to reach our ___3___.\n",
    "3. Eventually, with dedication, we reach the ___4___ outcome we aspired for.\n",
    "\"\"\"\n",
    "\n",
    "# Function to create fill-in-the-blanks puzzle\n",
    "def create_fill_in_the_blanks(text_template, path_keywords):\n",
    "    blanks = [f\"___{i}___\" for i, _ in enumerate(path_keywords, start=1)]\n",
    "    for blank, keyword in zip(blanks, path_keywords):\n",
    "        text_template = text_template.replace(blank, \"__________\")\n",
    "    return text_template\n",
    "# Function to ask user to fill in the blanks and check against the shortest path\n",
    "def solve_fill_in_the_blanks(puzzle_text, path_keywords):\n",
    "    user_answers = []\n",
    "    for i, keyword in enumerate(path_keywords, start=1):\n",
    "        # User input for each blank\n",
    "        user_input = input(f\"Enter the word for blank {i}: \").strip().lower()\n",
    "        user_answers.append(user_input)\n",
    "    \n",
    "    # Validate user answers\n",
    "    correct_answers = 0\n",
    "    for user_answer, correct_answer in zip(user_answers, path_keywords):\n",
    "        if user_answer == correct_answer:\n",
    "            correct_answers += 1\n",
    "        else:\n",
    "            print(f\"The correct word for blank was '{correct_answer}', you entered '{user_answer}'.\")\n",
    "\n",
    "    print(f\"\\nYou got {correct_answers} out of {len(path_keywords)} correct!\")\n",
    "\n",
    "# Create puzzle text\n",
    "puzzle_text = create_fill_in_the_blanks(text_template, shortest_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc139179",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(puzzle_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e1afe7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "solve_fill_in_the_blanks(puzzle_text, shortest_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a362878",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Assuming 'text' is your preprocessed text with sentences.\n",
    "text = \"\"\"\n",
    "The journey of self-improvement is always ongoing, striving from being good to achieving greatness.\n",
    "A moment of reflection can sometimes show us the direction we need to take to reach our goal.\n",
    "Eventually, with dedication, we reach the great outcome we aspired for.\n",
    "\"\"\"\n",
    "\n",
    "# Keywords from the shortest path to create questions\n",
    "keywords = ['good', 'reflection', 'goal', 'great']\n",
    "\n",
    "# Function to replace keywords with blanks in text\n",
    "def replace_keywords_with_blanks(text, keywords):\n",
    "    for keyword in keywords:\n",
    "        text = re.sub(f\"\\\\b{keyword}\\\\b\", '__________', text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "# Function to display the puzzle with blanks and then reveal the answers\n",
    "def display_puzzle_with_answers(text, keywords):\n",
    "    puzzle_text = replace_keywords_with_blanks(text, keywords)\n",
    "    print(\"Fill-in-the-Blanks Puzzle:\\n\")\n",
    "    print(puzzle_text)\n",
    "    print(\"\\nAnswers:\")\n",
    "    for keyword in keywords:\n",
    "        print(f\"The word for the blank is: {keyword}\")\n",
    "\n",
    "# Generate the puzzle\n",
    "display_puzzle_with_answers(text, keywords)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb9e15a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c635ffa1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d854e77",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5a5e6b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
